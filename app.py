import os
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

import streamlit as st
import pandas as pd
from datetime import datetime

from src.literature.db_manager import (
    LiteratureDatabaseManager,
    create_literature_database,
)
from src.draft.analyzer import DraftAnalyzer
from src.citation.ai_matcher import (
    AICitationMatcher,
    AIAPIManager,
    SentenceWithAICitations,
)
from src.citation.format_learner import ReferenceFormatLearner
from src.utils.config import get_config


def init_session_state():
    """åˆå§‹åŒ–session state"""
    if "db_manager" not in st.session_state:
        st.session_state.db_manager = None
    if "draft_analysis" not in st.session_state:
        st.session_state.draft_analysis = None
    if "citation_results" not in st.session_state:
        st.session_state.citation_results = None
    if "imported_files" not in st.session_state:
        st.session_state.imported_files = []


def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        st.title("ğŸ“– è®ºæ–‡åæ’åŠ©æ‰‹")
        st.caption("åŸºäºAIçš„å­¦æœ¯è®ºæ–‡å¼•ç”¨è‡ªåŠ¨æ’å…¥å·¥å…·")

        st.divider()

        # 1. APIè®¾ç½®ï¼ˆå¯æŠ˜å ï¼‰
        with st.expander("ğŸ”‘ APIé…ç½®", expanded=True):
            api_provider = st.selectbox(
                "APIæä¾›å•†",
                options=["deepseek", "openai", "anthropic"],
                index=0,
                help="é€‰æ‹©AIæ¨¡å‹æä¾›å•†",
            )

            if api_provider == "deepseek":
                api_key = st.text_input(
                    "APIå¯†é’¥",
                    value="",
                    type="password",
                    placeholder="sk-...",
                    help="DeepSeek APIå¯†é’¥",
                )
                api_base_url = "https://api.deepseek.com/v1"
                model = st.selectbox(
                    "æ¨¡å‹",
                    options=["deepseek-chat", "deepseek-reasoner"],
                    index=0,
                )
            elif api_provider == "openai":
                api_key = st.text_input(
                    "APIå¯†é’¥",
                    value="",
                    type="password",
                    placeholder="sk-...",
                )
                api_base_url = st.text_input(
                    "APIåœ°å€ï¼ˆå¯é€‰ï¼‰",
                    value="",
                    placeholder="è‡ªå®šä¹‰ä¸­è½¬åœ°å€",
                )
                model = st.selectbox(
                    "æ¨¡å‹",
                    options=["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"],
                    index=0,
                )
            else:
                api_key = st.text_input(
                    "APIå¯†é’¥",
                    value="",
                    type="password",
                    placeholder="sk-ant-...",
                )
                api_base_url = ""
                model = st.selectbox(
                    "æ¨¡å‹",
                    options=["claude-3-5-sonnet-20241022"],
                    index=0,
                )

            # APIçŠ¶æ€æ£€æŸ¥
            if api_key:
                st.success("âœ… APIå·²é…ç½®")
            else:
                st.warning("âš ï¸ è¯·è¾“å…¥APIå¯†é’¥")

        # 2. å¼•ç”¨è®¾ç½®
        with st.expander("ğŸ“š å¼•ç”¨è®¾ç½®"):
            citation_style = st.selectbox(
                "å¼•ç”¨é£æ ¼",
                options=["author-year", "numbered"],
                index=0,
                help="é€‰æ‹©æ–‡ä¸­å¼•ç”¨æ ¼å¼",
            )

            max_citations = st.slider(
                "æ¯å¥æœ€å¤§å¼•ç”¨æ•°",
                min_value=1,
                max_value=5,
                value=2,
            )

            min_relevance = st.slider(
                "æœ€ä½ç›¸å…³æ€§é˜ˆå€¼",
                min_value=0.0,
                max_value=1.0,
                value=0.6,
                step=0.05,
                help="ä½äºæ­¤åˆ†æ•°çš„å¼•ç”¨å°†è¢«å¿½ç•¥",
            )

            st.caption(f"å½“å‰é˜ˆå€¼: {min_relevance:.2f} - ä½äºæ­¤åˆ†æ•°çš„å¼•ç”¨å°†è¢«è¿‡æ»¤")

        # 3. æ£€ç´¢å¼•æ“è®¾ç½®
        with st.expander("ğŸ” æ£€ç´¢å¼•æ“"):
            use_hybrid_search = st.toggle(
                "å¯ç”¨æ··åˆæ£€ç´¢",
                value=True,
                help="å¯ç”¨AIå¢å¼ºçš„æ··åˆæ£€ç´¢ï¼ˆéœ€è¦æ¨¡å‹æ–‡ä»¶ï¼‰",
            )

            if use_hybrid_search:
                st.markdown(
                    """
                <small style='color:green'>âœ… æŸ¥è¯¢æ‰©å±• â†’ å¤šè·¯å¬å› â†’ Cross-encoderé‡æ’ â†’ MMRå¤šæ ·</small>
                """,
                    unsafe_allow_html=True,
                )
            else:
                st.markdown(
                    """
                <small style='color:orange'>âš ï¸ ä»…ä½¿ç”¨å…³é”®è¯æ£€ç´¢</small>
                """,
                    unsafe_allow_html=True,
                )

        # 4. æ–‡çŒ®ç­›é€‰ç­–ç•¥
        with st.expander("âš–ï¸ æ–‡çŒ®ç­›é€‰ç­–ç•¥"):
            st.markdown("**ä¸¤æ­¥ç­›é€‰æ³•**")
            st.caption("1. è¯­ä¹‰ç­›é€‰ â†’ 2. æ–°é¢–åº¦/å¼•ç”¨åŠ æƒæ’åº")

            top_k_semantic = st.slider(
                "è¯­ä¹‰ç­›é€‰ä¿ç•™æ•°é‡",
                min_value=10,
                max_value=100,
                value=50,
                step=10,
            )

            st.divider()

            # æƒé‡æ»‘å—
            col_w1, col_w2 = st.columns(2)
            with col_w1:
                weight_recency = st.slider(
                    "ğŸ“… æ–°é¢–åº¦",
                    0,
                    100,
                    50,
                    step=5,
                )
            with col_w2:
                weight_citation = 100 - weight_recency
                st.metric("ğŸ“š å¼•ç”¨", f"{weight_citation}%")

            # å¯è§†åŒ–æƒé‡
            st.progress(weight_recency / 100)
            st.caption(f"æ–°é¢–åº¦ {weight_recency}% | å¼•ç”¨ {weight_citation}%")

            # é¢„è®¾æŒ‰é’®
            preset_col1, preset_col2 = st.columns(2)
            with preset_col1:
                if st.button("âš–ï¸ å‡è¡¡", use_container_width=True):
                    weight_recency = 50
                    st.rerun()
            with preset_col2:
                if st.button("ğŸ†• è¿½æ–°", use_container_width=True):
                    weight_recency = 80
                    st.rerun()

        # 5. å‚è€ƒæ–‡çŒ®æ ¼å¼ï¼ˆå¯é€‰ï¼‰
        with st.expander("ğŸ“ å‚è€ƒæ–‡çŒ®æ ¼å¼", expanded=False):
            st.caption("ç²˜è´´ç›®æ ‡æœŸåˆŠçš„å‚è€ƒæ–‡çŒ®ç¤ºä¾‹ï¼ŒAIå°†å­¦ä¹ æ ¼å¼")

            reference_example = st.text_area(
                "ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰",
                height=100,
                placeholder="[1] Zhang, X. (2024). Title...",
            )

            if reference_example and st.button("ğŸ“ å­¦ä¹ æ ¼å¼", type="secondary"):
                if api_key:
                    with st.spinner("å­¦ä¹ ä¸­..."):
                        temp_api_manager = AIAPIManager(
                            api_key=api_key,
                            base_url=api_base_url or "https://api.deepseek.com/v1",
                            model=model,
                            provider=api_provider,
                        )
                        format_learner = ReferenceFormatLearner(temp_api_manager)
                        learned_format = format_learner.learn_from_example(
                            reference_example
                        )
                        st.session_state.reference_format = learned_format
                        st.success(f"âœ… å·²å­¦ä¹ : {learned_format.name}")
                else:
                    st.warning("âš ï¸ è¯·å…ˆé…ç½®APIå¯†é’¥")

        st.divider()

        # æ•°æ®åº“çŠ¶æ€
        if st.session_state.db_manager:
            stats = st.session_state.db_manager.get_statistics()
            st.markdown(
                f"""
            <div style='padding:10px; background: #f0f2f6; border-radius:10px;'>
                <b>ğŸ“Š æ•°æ®åº“çŠ¶æ€</b><br>
                æ–‡çŒ®æ•°é‡: <b>{stats["total_papers"]}</b><br>
                æœ€æ—©æ–‡çŒ®: <b>{stats["earliest_year"]}</b><br>
                æœ€æ–°æ–‡çŒ®: <b>{stats["latest_year"]}</b>
            </div>
            """,
                unsafe_allow_html=True,
            )

        return {
            "api_provider": api_provider,
            "api_key": api_key,
            "api_base_url": api_base_url,
            "model": model,
            "citation_style": citation_style,
            "max_citations": max_citations,
            "min_relevance": min_relevance,
            "top_k_semantic": top_k_semantic,
            "weight_recency": weight_recency,
            "weight_citation": weight_citation,
            "reference_example": reference_example,
            "use_hybrid_search": use_hybrid_search,
        }


def render_literature_import():
    """æ¸²æŸ“æ–‡çŒ®å¯¼å…¥Tab"""
    st.markdown("### ğŸ“š å¯¼å…¥æ–‡çŒ®åº“")

    # ç®€æ´çš„æ“ä½œè¯´æ˜
    with st.expander("ğŸ“‹ æ“ä½œæŒ‡å—", expanded=True):
        st.markdown("""
        **ä»Web of Scienceå¯¼å…¥ï¼š**
        1. åœ¨Web of Scienceä¸­æœç´¢æ–‡çŒ® â†’ 2. é€‰æ‹©è¦å¯¼å‡ºçš„æ–‡çŒ®
        3. ç‚¹å‡» **Export** â†’ **Plain Text File** 
        4. é€‰æ‹© **Full Record** æ ¼å¼ â†’ 5. ä¸‹è½½ .txt æ–‡ä»¶
        6. åœ¨ä¸‹æ–¹ä¸Šä¼ æ–‡ä»¶
        """)

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.markdown("**ä¸Šä¼ WOSå¯¼å‡ºæ–‡ä»¶**")
    uploaded_files = st.file_uploader(
        "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„æˆ–ç‚¹å‡»é€‰æ‹©",
        type=["txt"],
        accept_multiple_files=True,
        help="æ”¯æŒæ‰¹é‡ä¸Šä¼ å¤šä¸ªtxtæ–‡ä»¶",
    )

    if uploaded_files:
        # æ–‡ä»¶åˆ—è¡¨æ˜¾ç¤º
        st.success(f"âœ… å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

        # æ˜¾ç¤ºæ–‡ä»¶å
        with st.expander(f"æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨ ({len(uploaded_files)}ä¸ª)"):
            for f in uploaded_files:
                st.caption(f"ğŸ“„ {f.name}")

        if st.button("ğŸš€ å¼€å§‹å¯¼å…¥", type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()

            # åˆå§‹åŒ–æ•°æ®åº“
            db_path = "data/literature.db"
            db_manager = LiteratureDatabaseManager(db_path)

            total_count = 0
            all_errors = []

            for idx, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"æ­£åœ¨å¯¼å…¥: {uploaded_file.name}...")

                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                file_path = f"uploads/{uploaded_file.name}"
                os.makedirs("uploads", exist_ok=True)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())

                # å¯¼å…¥æ–‡çŒ®
                count, errors = db_manager.import_from_wos_txt(file_path)
                total_count += count
                all_errors.extend(errors)

                # è®°å½•å·²å¯¼å…¥çš„æ–‡ä»¶
                if uploaded_file.name not in st.session_state.imported_files:
                    st.session_state.imported_files.append(uploaded_file.name)

                progress_bar.progress((idx + 1) / len(uploaded_files))

            st.session_state.db_manager = db_manager

            # æ˜¾ç¤ºç»Ÿè®¡
            stats = db_manager.get_statistics()

            st.success(f"âœ… æˆåŠŸå¯¼å…¥ {total_count} ç¯‡è®ºæ–‡ï¼")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»æ–‡çŒ®æ•°", stats["total_papers"])
            with col2:
                years = list(stats["year_distribution"].keys())
                if years:
                    st.metric("å¹´ä»½èŒƒå›´", f"{min(years)}-{max(years)}")
            with col3:
                st.metric("æœŸåˆŠç§ç±»", len(stats["top_journals"]))

            if all_errors:
                with st.expander(f"æŸ¥çœ‹é”™è¯¯ ({len(all_errors)}ä¸ª)"):
                    for error in all_errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                        st.error(error)


def render_draft_upload():
    """æ¸²æŸ“è‰ç¨¿ä¸Šä¼ Tab"""
    st.markdown("### ğŸ“ ä¸Šä¼ è‰ç¨¿")

    # æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥æ–‡çŒ®
    if st.session_state.db_manager is None:
        st.warning('âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§"å¯¼å…¥æ–‡çŒ®åº“"ä¸­å¯¼å…¥æ–‡çŒ®')

        st.markdown("""
        ---
        **å¿«é€Ÿå¼€å§‹ï¼š**
        1. åˆ‡æ¢åˆ° **ğŸ“š å¯¼å…¥æ–‡çŒ®åº“** æ ‡ç­¾
        2. ä¸Šä¼ Web of Scienceå¯¼å‡ºçš„.txtæ–‡ä»¶
        3. ç­‰å¾…å¯¼å…¥å®Œæˆ
        """)
        return

    # æ˜¾ç¤ºå½“å‰æ–‡çŒ®åº“ä¿¡æ¯
    stats = st.session_state.db_manager.get_statistics()
    st.success(f"âœ… å·²åŠ è½½æ–‡çŒ®åº“: {stats['total_papers']} ç¯‡è®ºæ–‡")

    # æ–‡ä»¶ä¸Šä¼ 
    st.markdown("**ä¸Šä¼ Wordæ–‡æ¡£**")
    uploaded_file = st.file_uploader(
        "æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤å¤„æˆ–ç‚¹å‡»é€‰æ‹©",
        type=["docx"],
        help="ä¸Šä¼ å†™å¥½ä½†æœªæ’å…¥å¼•ç”¨çš„Wordæ–‡æ¡£ï¼ˆ.docxæ ¼å¼ï¼‰",
    )

    if uploaded_file:
        # æ–‡ä»¶ä¿¡æ¯
        st.info(f"ğŸ“„ {uploaded_file.name} ({(uploaded_file.size / 1024):.1f} KB)")

        # ä¿å­˜æ–‡ä»¶
        file_path = f"uploads/{uploaded_file.name}"
        os.makedirs("uploads", exist_ok=True)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        # åˆ†ææŒ‰é’®
        if st.button("ğŸ”¬ åˆ†ææ–‡æ¡£", type="primary"):
            with st.spinner("æ­£åœ¨åˆ†ææ–‡æ¡£ç»“æ„..."):
                analyzer = DraftAnalyzer()
                analysis = analyzer.analyze_draft(file_path)
                st.session_state.draft_analysis = analysis

            # åˆ†æç»“æœç»Ÿè®¡
            st.success(f"âœ… åˆ†æå®Œæˆï¼")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»å¥å­æ•°", len(analysis.sentences))
            with col2:
                needing_citations = len(
                    [s for s in analysis.sentences if not s.has_citation]
                )
                st.metric("éœ€å¼•ç”¨å¥å­", needing_citations)
            with col3:
                st.metric("æ®µè½æ•°", len(analysis.paragraphs))

            # æ˜¾ç¤ºå‰å‡ ä¸ªå¥å­
            with st.expander("é¢„è§ˆå¥å­"):
                for i, sent in enumerate(analysis.sentences[:5]):
                    st.markdown(f"**å¥å­ {i + 1}:** {sent.text[:100]}...")
                    if sent.keywords:
                        st.caption(f"å…³é”®è¯: {', '.join(sent.keywords)}")


def render_citation_matching(config):
    """æ¸²æŸ“å¼•ç”¨åŒ¹é…Tab"""
    st.header("ğŸ” å¼•ç”¨åŒ¹é…")

    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if st.session_state.db_manager is None:
        st.warning("âš ï¸ è¯·å…ˆå¯¼å…¥æ–‡çŒ®åº“")
        return

    if st.session_state.draft_analysis is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶åˆ†æè‰ç¨¿")
        return

    analysis = st.session_state.draft_analysis

    # åŒ¹é…é€‰é¡¹
    col1, col2, col3 = st.columns(3)
    with col1:
        exclude_existing = st.checkbox(
            "è·³è¿‡å·²æœ‰å¼•ç”¨çš„å¥å­", value=True, help="ä¸å¤„ç†å·²ç»åŒ…å«å¼•ç”¨çš„å¥å­"
        )
    with col2:
        year_range = st.slider(
            "æ–‡çŒ®å¹´ä»½èŒƒå›´",
            min_value=5,
            max_value=30,
            value=10,
            help="åªæœç´¢æœ€è¿‘Nå¹´çš„æ–‡çŒ®",
        )
    with col3:
        prioritize_recent = st.checkbox(
            "ä¼˜å…ˆæ¨èæ–°æ–‡çŒ®", value=True, help="ä¼˜å…ˆåŒ¹é…è¿‘5å¹´çš„æ–‡çŒ®"
        )

    # æ£€æŸ¥APIé…ç½®
    if not config.get("api_key"):
        st.error("âš ï¸ è¯·åœ¨ä¾§è¾¹æ è¾“å…¥APIå¯†é’¥")
        return

    # å¼€å§‹åŒ¹é…
    if st.button("å¼€å§‹AIåŒ¹é…å¼•ç”¨", type="primary"):
        sentences_to_match = analysis.sentences
        if exclude_existing:
            sentences_to_match = [s for s in analysis.sentences if not s.has_citation]

        if not sentences_to_match:
            st.warning("æ²¡æœ‰éœ€è¦åŒ¹é…çš„å¥å­")
            return

        # åˆå§‹åŒ–AI APIç®¡ç†å™¨
        api_manager = AIAPIManager(
            api_key=config["api_key"],
            base_url=config.get("api_base_url", "https://api.deepseek.com/v1"),
            model=config.get("model", "deepseek-chat"),
            provider=config.get("api_provider", "deepseek"),
        )

        # åˆå§‹åŒ–AIåŒ¹é…å™¨ï¼ˆä¼ å…¥ç”¨æˆ·è®¾ç½®çš„å‚æ•°ï¼‰
        matcher = AICitationMatcher(
            db_manager=st.session_state.db_manager,
            api_manager=api_manager,
            citation_style=config["citation_style"],
            max_citations=config["max_citations"],
            min_relevance=config.get("min_relevance", 0.6),
            batch_size=5,
            top_k_semantic=int(config.get("top_k_semantic", 50)),
            weight_recency=int(config.get("weight_recency", 50)),
            weight_citation=int(config.get("weight_citation", 50)),
            use_hybrid_search=config.get("use_hybrid_search", True),
        )

        st.info("ğŸ¤– æ­£åœ¨ä½¿ç”¨AIè¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")

        # è¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()

        def progress_callback(current, total):
            progress_bar.progress(current / total)
            status_text.text(f"æ­£åœ¨AIåŒ¹é…: å¥å­ {current}/{total}")

        # æ‰¹é‡åŒ¹é…
        results = matcher.batch_match(
            sentences=sentences_to_match,
            year_range=year_range,
            progress_callback=progress_callback,
        )

        st.session_state.citation_results = results
        st.session_state.citation_matcher = matcher

        st.success(f"âœ… AIåŒ¹é…å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªå¥å­")

        # ç»Ÿè®¡
        with_citations = len([r for r in results if r.citations])
        high_confidence = len(
            [r for r in results if any(c.confidence == "high" for c in r.citations)]
        )

        # ç»Ÿè®¡è¿‘5å¹´æ–‡çŒ®å æ¯”
        current_year = datetime.now().year
        recent_papers = 0
        total_papers = 0
        for r in results:
            for c in r.citations:
                total_papers += 1
                if c.paper.year >= current_year - 5:
                    recent_papers += 1

        recent_ratio = (recent_papers / total_papers * 100) if total_papers > 0 else 0

        # ç»Ÿè®¡å¡ç‰‡ï¼ˆæ›´ç¾è§‚çš„æ˜¾ç¤ºï¼‰
        st.markdown("### ğŸ“ˆ åŒ¹é…ç»Ÿè®¡")

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("âœ… æˆåŠŸåŒ¹é…", f"{with_citations}/{len(results)}")
        with col2:
            st.metric("ğŸŸ¢ é«˜ç½®ä¿¡åº¦", high_confidence)
        with col3:
            st.metric("ğŸ“… è¿‘5å¹´æ–‡çŒ®", f"{recent_ratio:.0f}%")
        with col4:
            match_rate = with_citations / len(results) * 100 if len(results) > 0 else 0
            st.metric("ğŸ“Š åŒ¹é…ç‡", f"{match_rate:.1f}%")

        # è¿›åº¦æ¡å¯è§†åŒ–
        if total_papers > 0:
            st.progress(match_rate / 100)
            st.caption(f"å·²ä¸º {with_citations} ä¸ªå¥å­æ‰¾åˆ°åˆé€‚çš„å¼•ç”¨")


def render_results_review(config):
    """æ¸²æŸ“ç»“æœæŸ¥çœ‹Tab"""
    st.header("ğŸ“Š æŸ¥çœ‹ä¸å¯¼å‡º")

    if st.session_state.citation_results is None:
        st.warning("âš ï¸ è¯·å…ˆå®Œæˆå¼•ç”¨åŒ¹é…")
        return

    results = st.session_state.citation_results
    matcher = st.session_state.get("citation_matcher")

    # å¯¼å‡ºé€‰é¡¹
    st.subheader("å¯¼å‡ºè®¾ç½®")
    col1, col2 = st.columns(2)
    with col1:
        output_format = st.selectbox(
            "è¾“å‡ºæ ¼å¼", options=["Wordæ–‡æ¡£", "Markdown", "çº¯æ–‡æœ¬"], index=0
        )
    with col2:
        bibliography_style = st.selectbox(
            "å‚è€ƒæ–‡çŒ®æ ¼å¼", options=["apa", "nature", "vancouver", "ieee"], index=0
        )

    # æ˜¾ç¤ºå½“å‰ç­›é€‰ç­–ç•¥é…ç½®
    with st.expander("ğŸ“Š å½“å‰ç­›é€‰ç­–ç•¥", expanded=True):
        top_k = config.get("top_k_semantic", 50)
        w_rec = config.get("weight_recency", 50)
        w_cit = config.get("weight_citation", 50)

        st.markdown(f"**ç¬¬ä¸€æ­¥ï¼šè¯­ä¹‰ç­›é€‰** - é€‰å‡ºæœ€ç›¸å…³çš„å‰ **{top_k}** ç¯‡")

        st.markdown("**ç¬¬äºŒæ­¥ï¼šåŠ æƒæ’åº**")
        col_w1, col_w2 = st.columns(2)
        with col_w1:
            st.metric("ğŸ“… æ–°é¢–åº¦æƒé‡", f"{w_rec}%")
        with col_w2:
            st.metric("ğŸ“š å¼•ç”¨æ¬¡æ•°æƒé‡", f"{w_cit}%")

        if w_rec + w_cit == 100:
            st.success("âœ… æƒé‡åˆ†é…æ­£ç¡®")

    st.divider()

    # æ˜¾ç¤ºåŒ¹é…ç»“æœ
    st.subheader("åŒ¹é…ç»“æœè¯¦æƒ…")

    # è¿‡æ»¤é€‰é¡¹
    show_only_with_citations = st.checkbox("åªæ˜¾ç¤ºæœ‰å¼•ç”¨çš„å¥å­", value=False)

    display_results = results
    if show_only_with_citations:
        display_results = [r for r in results if r.citations]

    # åˆ†é¡µæ˜¾ç¤º
    page_size = 10
    total_pages = max(1, (len(display_results) + page_size - 1) // page_size)
    page = (
        st.number_input(
            f"é¡µç  (å…±{total_pages}é¡µ)", min_value=1, max_value=total_pages, value=1
        )
        - 1
    )

    start_idx = page * page_size
    end_idx = min(start_idx + page_size, len(display_results))

    for i, result in enumerate(display_results[start_idx:end_idx], start=start_idx + 1):
        with st.container():
            st.markdown(f"**å¥å­ {i}**")
            st.info(result.sentence.text)

            if result.sentence.has_citation:
                st.success(f"âœ“ å·²æœ‰å¼•ç”¨: {result.sentence.citation_text}")
            elif result.citations:
                st.markdown("**AIæ¨èå¼•ç”¨:**")
                for j, citation in enumerate(result.citations, 1):
                    paper = citation.paper
                    col1, col2 = st.columns([3, 1])

                    # è®¡ç®—å¹´ä»½æ ‡ç­¾
                    current_year_now = datetime.now().year
                    year_diff = current_year_now - paper.year
                    if year_diff <= 2:
                        year_badge = "ğŸ”¥ æœ€æ–°"
                    elif year_diff <= 5:
                        year_badge = "â­ è¿‘5å¹´"
                    elif year_diff <= 10:
                        year_badge = "ğŸ“š è¿‘10å¹´"
                    else:
                        year_badge = "ğŸ“– ç»å…¸"

                    with col1:
                        st.markdown(f"{j}. **{paper.title}**")
                        st.caption(f"ä½œè€…: {paper.authors[:100]}...")
                        st.caption(
                            f"æœŸåˆŠ: {paper.journal} | {year_badge} ({paper.year}) | è¢«å¼•: {paper.cited_by}æ¬¡"
                        )
                        confidence_emoji = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}
                        emoji = confidence_emoji.get(citation.confidence, "âšª")

                        # AIè¯„åˆ†å’Œç½®ä¿¡åº¦
                        score_color = (
                            "green"
                            if citation.relevance_score >= 0.75
                            else "orange"
                            if citation.relevance_score >= 0.5
                            else "red"
                        )
                        st.markdown(
                            f"<span style='color:{score_color}'>{emoji} AIè¯„åˆ†: {citation.relevance_score:.2f}</span> "
                            f"<span style='color:gray'>(ç½®ä¿¡åº¦: {citation.confidence})</span>",
                            unsafe_allow_html=True,
                        )

                        # åŒ¹é…ç†ç”± - æ›´è¯¦ç»†çš„æ˜¾ç¤º
                        if citation.relevance_reason:
                            with st.expander("ğŸ“ æŸ¥çœ‹åŒ¹é…ç†ç”±", expanded=False):
                                st.markdown(f"_{citation.relevance_reason}_")
                    with col2:
                        cite_text = (
                            matcher.format_citation(citation, j)
                            if matcher
                            else f"[{j}]"
                        )
                        st.code(cite_text)
            else:
                st.warning("AIæœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")

            st.divider()

    # å‚è€ƒæ–‡çŒ®åºå·æ ¼å¼è®¾ç½®
    st.subheader("å‚è€ƒæ–‡çŒ®åºå·æ ¼å¼")
    ref_numbering = st.radio(
        "é€‰æ‹©åºå·æ ¼å¼",
        options=["numbered", "none", "author_year"],
        format_func=lambda x: {
            "numbered": "[1], [2], [3]...",
            "none": "æ— åºå·ï¼ˆç›´æ¥åˆ—å‡ºï¼‰",
            "author_year": "(Author, Year)",
        }.get(x, x),
        help="é€‰æ‹©å‚è€ƒæ–‡çŒ®åˆ—è¡¨çš„ç¼–å·æ–¹å¼",
    )

    # å¯¼å‡ºæŒ‰é’®
    st.subheader("å¯¼å‡ºæ–‡æ¡£")

    if st.button("ç”Ÿæˆå¸¦å¼•ç”¨çš„æ–‡æ¡£", type="primary"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡æ¡£..."):
            # ä¿æŒæ®µè½ç»“æ„é‡å»ºæ–‡æ¡£
            from src.draft.analyzer import DraftAnalyzer

            # æ„å»ºæ®µè½æ˜ å°„ï¼ˆæŒ‰æ®µè½ç´¢å¼•ç»„ç»‡å¥å­ï¼‰
            paragraph_map = {}
            for result in results:
                para_idx = result.sentence.paragraph_index
                if para_idx not in paragraph_map:
                    paragraph_map[para_idx] = []
                paragraph_map[para_idx].append(result)

            # æŒ‰æ®µè½é‡å»ºæ–‡æœ¬
            paragraphs_text = []
            for para_idx in sorted(paragraph_map.keys()):
                para_sentences = paragraph_map[para_idx]
                para_text_parts = []

                for result in para_sentences:
                    if (
                        result.citations
                        and not result.sentence.has_citation
                        and matcher
                    ):
                        # æ’å…¥å¼•ç”¨
                        new_text = matcher.insert_citations_into_text(
                            result.sentence, result.citations
                        )
                        para_text_parts.append(new_text)
                    else:
                        para_text_parts.append(result.sentence.text)

                # ç»„åˆæˆæ®µè½ï¼ˆä¿ç•™åŸæ®µè½ç»“æ„ï¼‰
                paragraph_text = " ".join(para_text_parts)
                paragraphs_text.append(paragraph_text)

            # ç”¨æ®µè½åˆ†éš”ç¬¦è¿æ¥
            full_text = "\n\n".join(paragraphs_text)

            # æ·»åŠ å‚è€ƒæ–‡çŒ®
            if matcher:
                # æ£€æŸ¥æ˜¯å¦æœ‰å­¦ä¹ çš„æ ¼å¼
                learned_format = st.session_state.get("reference_format")
                if learned_format and config.get("api_key"):
                    # ä½¿ç”¨å­¦ä¹ çš„æ ¼å¼
                    with st.spinner("æ­£åœ¨ä½¿ç”¨å­¦ä¹ åˆ°çš„æ ¼å¼ç”Ÿæˆå‚è€ƒæ–‡çŒ®..."):
                        api_manager = AIAPIManager(
                            api_key=config["api_key"],
                            base_url=config.get(
                                "api_base_url", "https://api.deepseek.com/v1"
                            ),
                            model=config.get("model", "deepseek-chat"),
                            provider=config.get("api_provider", "deepseek"),
                        )
                        format_learner = ReferenceFormatLearner(api_manager)
                        format_learner.format_cache = learned_format

                        # æ”¶é›†æ‰€æœ‰ä½¿ç”¨è¿‡çš„è®ºæ–‡
                        used_papers = {}
                        for swc in results:
                            for citation in swc.citations:
                                paper_id = citation.paper.id
                                if paper_id not in used_papers:
                                    used_papers[paper_id] = citation.paper

                        # ä½¿ç”¨å­¦ä¹ çš„æ ¼å¼æ‰¹é‡æ ¼å¼åŒ–
                        sorted_papers = sorted(
                            used_papers.values(),
                            key=lambda p: (
                                p.authors.split(",")[0].strip().split()[-1]
                                if p.authors
                                else ""
                            ).lower(),
                        )

                        formatted_refs = format_learner.batch_format(sorted_papers)

                        # æ ¹æ®åºå·æ ¼å¼ç”Ÿæˆå‚è€ƒæ–‡çŒ®
                        if ref_numbering == "numbered":
                            bibliography = "# References\n\n" + "\n\n".join(
                                f"[{i + 1}] {ref}"
                                for i, ref in enumerate(formatted_refs)
                            )
                        elif ref_numbering == "author_year":
                            bibliography = "# References\n\n" + "\n\n".join(
                                formatted_refs
                            )
                        else:  # none
                            bibliography = "# References\n\n" + "\n\n".join(
                                formatted_refs
                            )
                else:
                    # ä½¿ç”¨é»˜è®¤æ ¼å¼
                    used_papers = {}
                    for swc in results:
                        for citation in swc.citations:
                            paper_id = citation.paper.id
                            if paper_id not in used_papers:
                                used_papers[paper_id] = citation.paper

                    if used_papers:
                        sorted_papers = sorted(
                            used_papers.values(),
                            key=lambda p: (
                                p.authors.split(",")[0].strip().split()[-1]
                                if p.authors
                                else ""
                            ).lower(),
                        )

                        # æ ¹æ®åºå·æ ¼å¼ç”Ÿæˆå‚è€ƒæ–‡çŒ®
                        if ref_numbering == "numbered":
                            bibliography = "# References\n\n"
                            for i, paper in enumerate(sorted_papers, 1):
                                authors = paper.authors.replace(";", ", ")
                                ref = f"[{i}] {authors} ({paper.year}). {paper.title}. {paper.journal}, {paper.volume}({paper.issue}), {paper.pages}."
                                bibliography += ref + "\n\n"
                        elif ref_numbering == "author_year":
                            bibliography = "# References\n\n"
                            for paper in sorted_papers:
                                authors = paper.authors.replace(";", ", ")
                                ref = f"{authors} ({paper.year}). {paper.title}. {paper.journal}, {paper.volume}({paper.issue}), {paper.pages}."
                                bibliography += ref + "\n\n"
                        else:  # none
                            bibliography = "# References\n\n"
                            for paper in sorted_papers:
                                authors = paper.authors.replace(";", ", ")
                                ref = f"{authors} ({paper.year}). {paper.title}. {paper.journal}, {paper.volume}({paper.issue}), {paper.pages}."
                                bibliography += ref + "\n\n"
                    else:
                        bibliography = "# References\n\næš‚æ— å¼•ç”¨æ–‡çŒ®"

                full_text += "\n\n" + bibliography.strip()

            # ç¡®ä¿outputç›®å½•å­˜åœ¨
            os.makedirs("output", exist_ok=True)

            # ä¿å­˜æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if output_format == "çº¯æ–‡æœ¬":
                output_path = f"output/cited_draft_{timestamp}.txt"
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(full_text)

                with open(output_path, "r", encoding="utf-8") as f:
                    st.download_button(
                        label="ä¸‹è½½æ–‡æœ¬æ–‡ä»¶",
                        data=f.read(),
                        file_name=f"cited_draft_{timestamp}.txt",
                        mime="text/plain",
                    )

            elif output_format == "Markdown":
                output_path = f"output/cited_draft_{timestamp}.md"
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(full_text)

                with open(output_path, "r", encoding="utf-8") as f:
                    st.download_button(
                        label="ä¸‹è½½Markdownæ–‡ä»¶",
                        data=f.read(),
                        file_name=f"cited_draft_{timestamp}.md",
                        mime="text/markdown",
                    )

            else:  # Wordæ–‡æ¡£
                from docx import Document
                from docx.shared import Pt
                from docx.oxml.ns import qn

                output_path = f"output/cited_draft_{timestamp}.docx"
                doc = Document()

                def set_times_new_roman(run):
                    """è®¾ç½®Times New Romanå­—ä½“"""
                    run.font.name = "Times New Roman"
                    run._element.rPr.rFonts.set(qn("w:eastAsia"), "Times New Roman")
                    run.font.size = Pt(12)

                # æ·»åŠ å†…å®¹ï¼ˆä¿æŒæ®µè½ç»“æ„ï¼‰
                for para_idx in sorted(paragraph_map.keys()):
                    para_sentences = paragraph_map[para_idx]
                    para_text_parts = []

                    for result in para_sentences:
                        if (
                            result.citations
                            and not result.sentence.has_citation
                            and matcher
                        ):
                            new_text = matcher.insert_citations_into_text(
                                result.sentence, result.citations
                            )
                            para_text_parts.append(new_text)
                        else:
                            para_text_parts.append(result.sentence.text)

                    # æ·»åŠ æ®µè½
                    paragraph_text = " ".join(para_text_parts)
                    p = doc.add_paragraph(paragraph_text)

                    # è®¾ç½®å­—ä½“
                    for run in p.runs:
                        set_times_new_roman(run)

                # æ·»åŠ å‚è€ƒæ–‡çŒ®
                if matcher:
                    doc.add_heading("References", level=1)

                    # æ”¶é›†æ‰€æœ‰ä½¿ç”¨è¿‡çš„è®ºæ–‡
                    used_papers = {}
                    for swc in results:
                        for citation in swc.citations:
                            paper_id = citation.paper.id
                            if paper_id not in used_papers:
                                used_papers[paper_id] = citation.paper

                    if used_papers:
                        sorted_papers = sorted(
                            used_papers.values(),
                            key=lambda p: (
                                p.authors.split(",")[0].strip().split()[-1]
                                if p.authors
                                else ""
                            ).lower(),
                        )

                        # æ£€æŸ¥æ˜¯å¦æœ‰å­¦ä¹ çš„æ ¼å¼
                        learned_format = st.session_state.get("reference_format")
                        if learned_format and config.get("api_key"):
                            # ä½¿ç”¨å­¦ä¹ çš„æ ¼å¼
                            api_manager = AIAPIManager(
                                api_key=config["api_key"],
                                base_url=config.get(
                                    "api_base_url", "https://api.deepseek.com/v1"
                                ),
                                model=config.get("model", "deepseek-chat"),
                                provider=config.get("api_provider", "deepseek"),
                            )
                            format_learner = ReferenceFormatLearner(api_manager)
                            format_learner.format_cache = learned_format
                            formatted_refs = format_learner.batch_format(sorted_papers)
                        else:
                            # ä½¿ç”¨é»˜è®¤æ ¼å¼
                            formatted_refs = []
                            for paper in sorted_papers:
                                authors = paper.authors.replace(";", ", ")
                                ref = f"{authors} ({paper.year}). {paper.title}. {paper.journal}, {paper.volume}({paper.issue}), {paper.pages}."
                                formatted_refs.append(ref)

                        # æ ¹æ®åºå·æ ¼å¼æ·»åŠ å‚è€ƒæ–‡çŒ®
                        for i, ref in enumerate(formatted_refs, 1):
                            if ref_numbering == "numbered":
                                p = doc.add_paragraph(f"[{i}] {ref}")
                            elif ref_numbering == "author_year":
                                # ä»å¼•ç”¨ä¸­æå–ä½œè€…-å¹´ä»½æ ¼å¼
                                p = doc.add_paragraph(ref)
                            else:  # none
                                p = doc.add_paragraph(ref)

                            # è®¾ç½®å­—ä½“
                            for run in p.runs:
                                set_times_new_roman(run)

                doc.save(output_path)

                with open(output_path, "rb") as f:
                    st.download_button(
                        label="ä¸‹è½½Wordæ–‡æ¡£",
                        data=f.read(),
                        file_name=f"cited_draft_{timestamp}.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    )

            st.success(f"âœ… æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="è®ºæ–‡åæ’åŠ©æ‰‹",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # é¡µé¢æ ·å¼
    st.markdown(
        """
    <style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        font-weight: 600;
        margin-top: 1rem;
    }
    .highlight-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background: #f0f2f6;
        margin: 0.5rem 0;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    # é¡µé¢æ ‡é¢˜
    st.markdown('<p class="main-header">ğŸ“š è®ºæ–‡åæ’åŠ©æ‰‹</p>', unsafe_allow_html=True)
    st.markdown("åŸºäºAIçš„æ™ºèƒ½å¼•ç”¨æ’å…¥å·¥å…· | ä¸Šä¼ æ–‡çŒ®åº“å’Œè‰ç¨¿ï¼Œè‡ªåŠ¨åŒ¹é…æœ€ç›¸å…³çš„å‚è€ƒæ–‡çŒ®")

    # åˆå§‹åŒ–session state
    init_session_state()

    # æ¸²æŸ“ä¾§è¾¹æ 
    config = render_sidebar()

    # åˆ›å»ºæ ‡ç­¾é¡µï¼ˆå¸¦å›¾æ ‡ï¼‰
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ“š å¯¼å…¥æ–‡çŒ®åº“", "ğŸ“ ä¸Šä¼ è‰ç¨¿", "âš¡ AIåŒ¹é…", "ğŸ“Š æŸ¥çœ‹ä¸å¯¼å‡º"]
    )

    with tab1:
        render_literature_import()

    with tab2:
        render_draft_upload()

    with tab3:
        render_citation_matching(config)

    with tab4:
        render_results_review(config)

    # é¡µè„š
    st.sidebar.markdown("---")
    st.sidebar.caption("è®ºæ–‡åæ’åŠ©æ‰‹ v1.0")


if __name__ == "__main__":
    main()
